Process

Data Preprocessing:

Variables:
Our y variable = "IS_SUCCESSFUL" column


ATTEMPT 1
The first attempt (Models/AlphabetSoupCharity1.h5) resulted in an accuracy score of 72.7%.

The hyperparameters used were:

layers = 2
layer1 = 8 neurons : activation function = ‘relu’
layer2 = 5 neurons : activation function = ‘relu'
epochs = 100

ATTEMPT 2
For my second attempt (Models/AlphabetSoupCharity.h5). This attempt resulted in an accuracy score of 72.8%.This was the highest accuracy score of the three models.

The hyperparameters used were:

layers = 2
layer1 = 10 neurons : activation function = ‘relu’
layer2 = 6 neurons : activation function = ‘relu’
epochs = 100

ATTEMPT 3
For my third attempt (Resources/AlphabetSoupCharity2.h5). I added one more layer This attempt resulted in an accuracy score of 72.8%.

The hyperparameters used were:

layers = 3
layer1 = 9 neurons : activation function = ‘relu’
layer2 = 18 neurons : activation function = ‘tanh’
layer3 = 27 neurons : activation function = ‘tanh’
epochs = 100

Summary
In three attempts, the model couldn't exceed a predictive accuracy of 72.8%. Even after hypertuning, there was negligible improvement. Considering an alternative classification model might be prudent to enhance predictions on the success of applicants funded by Alphabet Soup.